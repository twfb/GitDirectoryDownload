#!/usr/bin/python3

import re
import os
import sys
import requests
import time
import html
import threading

from bs4 import BeautifulSoup

# Proxy for download
proxies = {
    #        "http": "socks5://127.0.0.1:7891",
    #        "https": "socks5://127.0.0.1:7891",
}

# Download from raw url
download_from_raw = True

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66"
}

session = requests.Session()


def get(url, home_url=None):
    url = "{}{}".format(home_url, url) if home_url else url

    for i in range(5):
        try:
            time.sleep(0.5 * i)
            response = session.get(
                url,
                headers=headers,
                proxies=proxies,
            )
            if response.status_code % 100 < 4:
                return response
        except:
            pass
    print("Cannot get {}".format(url))


def get_file_path(home_url, dir_path):
    soup = BeautifulSoup(get(dir_path, home_url=home_url).text, "html.parser")
    for div in soup.select('div[role="rowheader"]'):
        a = div.select_one("a")
        file_name = a.text.strip()
        if file_name != ".." and file_name != ".â€Š.":
            yield dir_path + "/" + file_name


def download_file(home_url, file_path):
    response = get(file_path, home_url=home_url)

    if not response:
        print("Download {} Failed".format(file_path))
        return

    file_html = response.text
    soup = BeautifulSoup(file_html, "html.parser")
    raw_url = soup.select_one("#raw-url")
    if not raw_url:
        return download_dir(home_url, file_path)

    print("Downloading {}".format(file_path))
    if download_from_raw:
        response = get(raw_url["href"], home_url="https://github.com")
        if not response:
            return
        content = response.content
    else:
        table_html = "\n".join(
            re.findall('(<td id="LC\d+".*?>.*?</td>)', file_html, re.S)
        )
        soup = BeautifulSoup("<pre>{}</pre>".format(table_html), "html.parser")
        content = "\n".join(
            [
                td.text if td.text.strip("\t ") != "\n" else ""
                for td in soup.select("td")
            ]
        )
        content = content.encode("utf8")

    with open(file_path, "wb") as f:
        f.write(content)
    print("Downloaded {}".format(file_path))


def download_dir(home_url, dir_path):
    os.makedirs(dir_path, exist_ok=True)
    for file_path in get_file_path(home_url, dir_path):
        threading.Thread(target=download_file, args=(home_url, file_path)).start()


def main():
    for dir_url in sys.argv[1:]:
        dir_url = dir_url.strip("/")
        dir_index = dir_url.rindex("/") + 1
        home_url = dir_url[:dir_index]
        dir_path = dir_url[dir_index:]
        download_dir(home_url, dir_path)


if __name__ == "__main__":
    main()
