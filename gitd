#!/bin/python3

import re
import os
import sys
import requests
import time
import html
import threading

from bs4 import BeautifulSoup

# Proxy for download
proxies = {
        "http": "socks5:// 172.23.64.1:9090",
        "https": "socks5://172.23.64.1:9090",
}

# Request url sleep time
sleep_time = 0.1

# Download from raw url
download_from_raw = True

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36 Edg/87.0.664.66"
}

session = requests.Session()


def get(url, home_url=None, sleep_time=sleep_time):
    time.sleep(sleep_time)
    url = "{}{}".format(home_url, url) if home_url else url
    if download_from_raw:
        url = url.replace('blob', 'raw')
    return session.get(
        url,
        headers=headers,
        proxies=proxies,
    )


def get_file_path(home_url, dir_path):
    soup = BeautifulSoup(get(dir_path, home_url=home_url).text, "html.parser")
    for div in soup.select('div[role="rowheader"]'):
        a = div.select_one("a")
        file_name = a.text.strip()
        if file_name != ".." and file_name != ".â€Š.":
            yield dir_path + '/' + file_name


def download_file(home_url, file_path):
    response = get(file_path, home_url=home_url)

    if response.status_code % 100 > 3:
        response = get(file_path, home_url=home_url, sleep_time=sleep_time + 1)
        if response.status_code % 100 > 3:
            print('Download {} Failed'.format(file_path))
            return
    file_html = response.text
    soup = BeautifulSoup(file_html, "html.parser")
    raw_url = soup.select_one("#raw-url")
    if not raw_url:
        return download_dir(home_url, file_path)

    print("Downloading {}".format(file_path))
    if download_from_raw:
        content = get(raw_url["href"], home_url=home_url).content
    else:
        table_html = "\n".join(
            re.findall('(<td id="LC\d+".*?>.*?</td>)', file_html, re.S)
        )
        soup = BeautifulSoup("<pre>{}</pre>".format(table_html), "html.parser")
        content = "\n".join(
            [
                td.text if td.text.strip("\t ") != "\n" else ""
                for td in soup.select("td")
            ]
        )
        content = content.encode("utf8")

    with open(file_path, "wb") as f:
        f.write(content)
    print("Downloaded {}".format(file_path))


def download_dir(home_url, dir_path):
    os.makedirs(dir_path, exist_ok=True)
    for file_path in get_file_path(home_url, dir_path):
        threading.Thread(target=download_file, args=(home_url, file_path)).start()


def main():
    for dir_url in sys.argv[1:]:
        dir_url = dir_url.strip('/')
        dir_index = dir_url.rindex('/') + 1
        home_url = dir_url[:dir_index] 
        dir_path = dir_url[dir_index:]
        download_dir(home_url, dir_path)


if __name__ == "__main__":
    main()
